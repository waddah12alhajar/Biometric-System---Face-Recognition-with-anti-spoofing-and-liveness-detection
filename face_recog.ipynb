{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Refrences for code of Siamese network: https://keras.io/examples/vision/siamese_network/\n> Note: The code has been slightly modified before using in this project\n","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport random\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras import backend, layers, metrics\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ntf.__version__, np.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting random seeds to enable consistency while testing.\nrandom.seed(5)\nnp.random.seed(5)\ntf.random.set_seed(5)\n\nROOT = \"/kaggle/input/face-recognition-dataset/Extracted Faces/Extracted Faces\"\n# function to read an image and resize it\ndef read_image(index):\n    path = os.path.join(ROOT, index[0], index[1])\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image=cv2.resize(image, (128,128), interpolation = cv2.INTER_AREA)\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the data so that we can have a subset of people that have not been seen by the model\ndef split_dataset(directory, split=0.9):\n    folders = os.listdir(directory)\n    num_train = int(len(folders)*split)\n    \n    random.shuffle(folders)\n    \n    train_list, test_list = {}, {}\n    \n    # Creating Train-list\n    for folder in folders[:num_train]:\n        num_files = len(os.listdir(os.path.join(directory, folder)))\n        train_list[folder] = num_files\n    \n    # Creating Test-list\n    for folder in folders[num_train:]:\n        num_files = len(os.listdir(os.path.join(directory, folder)))\n        test_list[folder] = num_files  \n    \n    return train_list, test_list\n\ntrain_list, test_list = split_dataset(ROOT, split=0.9)\nprint(\"Length of training list:\", len(train_list))\nprint(\"Length of testing list :\", len(test_list))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function used to make triplets from the data, returns anchor, positive example and negative example\ndef create_triplets(directory, folder_list, max_files=10):\n    triplets = []\n    folders = list(folder_list.keys())\n    \n    for folder in folders:\n        path = os.path.join(directory, folder)\n        files = list(os.listdir(path))[:max_files]\n        num_files = len(files)\n        \n        for i in range(0,num_files-1):\n            for j in range(i+1, num_files):\n                anchor = (folder, f\"{i}.jpg\")\n                positive = (folder, f\"{j}.jpg\")\n\n                neg_folder = folder\n                while neg_folder == folder:\n                    neg_folder = random.choice(folders)\n                neg_file = random.randint(0, folder_list[neg_folder]-1)\n                negative = (neg_folder, f\"{neg_file}.jpg\")\n\n                triplets.append((anchor, positive, negative))\n            \n    random.shuffle(triplets)\n    return triplets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_triplet = create_triplets(ROOT, train_list)\ntest_triplet  = create_triplets(ROOT, test_list)\n\nprint(\"Number of training triplets:\", len(train_triplet))\nprint(\"Number of testing triplets :\", len(test_triplet))\n\nprint(\"\\nExamples of triplets:\")\nfor i in range(5):\n    print(train_triplet[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function used get batch for training or testing the network\ndef get_batch(triplet_list, batch_size=256, preprocess=True):\n    batch_steps = len(triplet_list)//batch_size\n    \n    for i in range(batch_steps+1):\n        anchor   = []\n        positive = []\n        negative = []\n        \n        j = i*batch_size\n        while j<(i+1)*batch_size and j<len(triplet_list):\n            a, p, n = triplet_list[j]\n            anchor.append(read_image(a))\n            positive.append(read_image(p))\n            negative.append(read_image(n))\n            j+=1\n            \n        anchor = np.array(anchor)\n        positive = np.array(positive)\n        negative = np.array(negative)\n        \n        if preprocess:\n            anchor = preprocess_input(anchor)\n            positive = preprocess_input(positive)\n            negative = preprocess_input(negative)\n        \n        yield ([anchor, positive, negative])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_plots = 6\n\nf, axes = plt.subplots(num_plots, 3, figsize=(15, 20))\n\nfor x in get_batch(train_triplet, batch_size=num_plots, preprocess=False):\n    a,p,n = x\n    for i in range(num_plots):\n        axes[i, 0].imshow(a[i])\n        axes[i, 1].imshow(p[i])\n        axes[i, 2].imshow(n[i])\n        i+=1\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function is used to get the ppre-trained encoder architecture from keras \n#and also we add some more layers on top of it \ndef get_encoder(input_shape):\n    \"\"\" Returns the image encoding model \"\"\"\n\n    pretrained_model = Xception(\n        input_shape=input_shape,\n        weights='imagenet',\n        include_top=False,\n        pooling='avg',\n    )\n    #we freeze the model weights asit is already trained for faster training\n    for i in range(len(pretrained_model.layers)-27):\n        pretrained_model.layers[i].trainable = False\n\n    encode_model = Sequential([\n        pretrained_model,\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation=\"relu\"),\n        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n    ], name=\"Encode_Model\")\n    return encode_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This layer is used to get distance between samples, it is used in loss function\nclass DistanceLayer(layers.Layer):\n    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, anchor, positive, negative):\n        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n        return (ap_distance, an_distance)\n    \n#Here we desgin a siemese network around a encoder\ndef get_siamese_network(input_shape = (128, 128, 3)):\n    encoder = get_encoder(input_shape)\n    \n    # Input Layers for the images\n    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n    \n    ## Generate the encodings (feature vectors) for the images\n    encoded_a = encoder(anchor_input)\n    encoded_p = encoder(positive_input)\n    encoded_n = encoder(negative_input)\n    \n    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n    distances = DistanceLayer()(\n        encoder(anchor_input),\n        encoder(positive_input),\n        encoder(negative_input)\n    )\n    \n    # Creating the Model\n    siamese_network = Model(\n        inputs  = [anchor_input, positive_input, negative_input],\n        outputs = distances,\n        name = \"Siamese_Network\"\n    )\n    return siamese_network\n\nsiamese_network = get_siamese_network()\nsiamese_network.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(siamese_network, show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Here we define our siamese model and how it will be trained\n#We define what will happen during the forward operation and also how loss function is applied\nclass SiameseModel(Model):\n    # Builds a Siamese model based on a base-model\n    def __init__(self, siamese_network, margin=1.0):\n        super(SiameseModel, self).__init__()\n        \n        self.margin = margin\n        self.siamese_network = siamese_network\n        self.loss_tracker = metrics.Mean(name=\"loss\")\n\n    def call(self, inputs):\n        return self.siamese_network(inputs)\n\n    def train_step(self, data):\n        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n        with tf.GradientTape() as tape:\n            loss = self._compute_loss(data)\n            \n        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n        \n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        loss = self._compute_loss(data)\n        \n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def _compute_loss(self, data):\n        # Get the two distances from the network, then compute the triplet loss\n        ap_distance, an_distance = self.siamese_network(data)\n        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n        return loss\n\n    @property\n    def metrics(self):\n        # We need to list our metrics so the reset_states() can be called automatically.\n        return [self.loss_tracker]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model = SiameseModel(siamese_network)\n\noptimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\nsiamese_model.compile(optimizer=optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function is used while training to get the test scores on test set\ndef test_on_triplets(batch_size = 256):\n    pos_scores, neg_scores = [], []\n\n    for data in get_batch(test_triplet, batch_size=batch_size):\n        prediction = siamese_model.predict(data)\n        pos_scores += list(prediction[0])\n        neg_scores += list(prediction[1])\n    \n    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)\n    ap_mean = np.mean(pos_scores)\n    an_mean = np.mean(neg_scores)\n    ap_stds = np.std(pos_scores)\n    an_stds = np.std(neg_scores)\n    \n    print(f\"Accuracy on test = {accuracy:.5f}\")\n    return (accuracy, ap_mean, an_mean, ap_stds, an_stds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training the model\nsave_all = False\nepochs = 30\nbatch_size = 128\n\nmax_acc = 0\ntrain_loss = []\ntest_metrics = []\n\nfor epoch in range(1, epochs+1):\n    t = time.time()\n    \n    # Training the model on train data\n    epoch_loss = []\n    for data in get_batch(train_triplet, batch_size=batch_size):\n        loss = siamese_model.train_on_batch(data)\n        epoch_loss.append(loss)\n    epoch_loss = sum(epoch_loss)/len(epoch_loss)\n    train_loss.append(epoch_loss)\n\n    print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n    print(f\"Loss on train    = {epoch_loss:.5f}\")\n    \n    # Testing the model on test data\n    metric = test_on_triplets(batch_size=batch_size)\n    test_metrics.append(metric)\n    accuracy = metric[0]\n    \n    # Saving the model weights\n    if save_all or accuracy>=max_acc:\n        siamese_model.save(\"siamese_model_final_wof/\",save_format='tf') \n        max_acc = accuracy\n\n# Saving the model after all epochs run\nsiamese_model.save(\"siamese_model_final_wof/\",save_format='tf') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# shutil.make_archive(\"/kaggle/working/wwfinal_siamese_model_final_wof_saved\", 'zip', \"/kaggle/working/siamese_model_final_wof\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting encoder which will be used to generate embeddings or features\ndef extract_encoder(model):\n    encoder = get_encoder((128, 128, 3))\n    i=0\n    for e_layer in model.layers[0].layers[3].layers:\n        layer_weight = e_layer.get_weights()\n        encoder.layers[i].set_weights(layer_weight)\n        i+=1\n    return encoder\n\nencoder = extract_encoder(siamese_model)\nencoder.save(\"encoder_ourfaces.h5\")\nencoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoder=tf.keras.models.load_model('/kaggle/input/siamese-encoder/encoder_siamese.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Functions to get FAR and FRR values (coded by student)\ndef get_FAR(face_list1, face_list2, threshold=1.7):\n    # Getting the encodings for the passed faces\n    tensor1 = encoder.predict(face_list1)\n    tensor2 = encoder.predict(face_list2)\n    distance = np.sum(np.square(tensor1-tensor2), axis=-1)\n    FAR = np.where(distance<=threshold, 1, 0)\n    return FAR\n\ndef get_FRR(face_list1, face_list2, threshold=1.3):\n    # Getting the encodings for the passed faces\n    tensor1 = encoder.predict(face_list1)\n    tensor2 = encoder.predict(face_list2)\n    distance = np.sum(np.square(tensor1-tensor2), axis=-1)\n    FRR = np.where(distance>threshold, 1, 0)\n    return FRR","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get FAR and FRR values for a certain threshold value intervals\npos_list = np.array([])\nneg_list = np.array([])\nFRR = []\nFAR= []\nthresold_vals=list(np.arange(1, 1.9+0.2, 0.2))\n\n\nfor data in get_batch(test_triplet, batch_size=256):\n        a, p, n = data\n        for th in thresold_vals:\n            pos_list = np.append(pos_list, get_FRR(a, p,threshold=th))\n            neg_list = np.append(neg_list, get_FAR(a, n,threshold=th))\n            FRR_rate=sum(pos_list)/len(pos_list)\n            FAR_rate=sum(neg_list)/len(neg_list)\n            FRR.append(FRR_rate)\n            FAR.append(FAR_rate)\n        break\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the FAR and FRR plots\nimport matplotlib.pyplot as plt\n\n# Your lists for False Rejection Rate (FRR) and False Acceptance Rate (FAR)\n\n# Your thresholds\nthresholds = thresold_vals\n\n# Create the plot\nplt.plot(thresholds, FRR, label='FRR')\nplt.plot(thresholds, FAR, label='FAR')\n\n# Add labels and legend\nplt.xlabel('Threshold')\nplt.ylabel('Rate')\nplt.legend()\nplt.title('FRR vs FAR on test subset')\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get embeddings for face to make face encoding Database","metadata":{}},{"cell_type":"code","source":"def get_folder_names(path):\n    return [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n\ndef get_images(n):\n    folder_names = get_folder_names('/kaggle/input/face-recog-test/Biometric_data/')\n    images=[]\n    for folder in folder_names:\n        images_in_folder = os.listdir(folder)\n        chosen_images = random.sample(images_in_folder, n)\n        images += [os.path.join(folder, image) for image in chosen_images]\n\n    return images\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings=[]\nnames=[]\nfor i in get_images(2):\n    ttimages=cv2.imread(i)\n    ttimages = cv2.cvtColor(ttimages, cv2.COLOR_BGR2RGB)\n    ttimages=cv2.resize(ttimages, (128,128), interpolation = cv2.INTER_AREA)\n    ttimages=np.expand_dims(ttimages, axis=0)\n    ttimages=preprocess_input(ttimages)\n    enc1=encoder(ttimages)\n    encodings.append(enc1[0])\n    names.append(os.path.basename(os.path.dirname(i)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distance = np.sum(np.square(encodings[5]-encodings[4]), axis=-1)\n# distance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encodings=np.stack(encodings, axis=0)\nnames=np.array(names)\nnp.savez('names_en.npz', names=names)\nnp.savez('encodings_en.npz', names=encodings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loaded_name = np.load('/kaggle/working/names.npz')\n# names_array = loaded_name['names']\n\n# loaded_enc = np.load('/kaggle/working/encodings.npz')\n# encoding = loaded_enc['names']\n# print(encoding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert model to ONNX format for faster inference","metadata":{}},{"cell_type":"code","source":"!pip install onnxruntime\n!pip install -U tf2onnx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (coded by student)\nimport tf2onnx\nimport onnxruntime as rt\n\nspec = (tf.TensorSpec((None, 128, 128, 3), tf.float32, name=\"input\"),)\noutput_path = \"face-recog_enc\" + \".onnx\"\n\nmodel_proto, _ = tf2onnx.convert.from_keras(encoder, input_signature=spec, opset=13, output_path=output_path)\noutput_names = [n.name for n in model_proto.graph.output]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"providers = ['CPUExecutionProvider']\nm = rt.InferenceSession('/kaggle/working/face-recog.onnx', providers=providers)\nonnx_pred = m.run(output_names, {\"input\": a})\n\n# print('ONNX Predicted:',onnx_pred)\n\n# make sure ONNX and keras have the same results\n# np.testing.assert_allclose(pred, onnx_pred[0], rtol=1e-5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocess function should be used mandatory for getting enccoding at inference\ndef c_preprocess_input(x):\n    x = x.astype('float32')\n    x /= 255.\n    x -= 0.5\n    x *= 2.\n    return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}